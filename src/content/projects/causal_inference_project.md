---
title: Does Empathetic AI Language Cause Emotional Attachment?
description: A causal inference study investigating whether empathetic language generated by large language models directly increases users' emotional attachment in real-world chatbot conversations.
image: /projects_images/causal_inference/llm-empathy-causal.jpg
tags: [Causal Inference, Large Language Models, Human-AI]
date: 2025-12-01
github: https://github.com/dangolofrancesco/llm-empathy-causal-study
report: /projects_images/causal_inference/Report.pdf
---

<div class="hidden md:block md:col-span-1"></div>

<div class="col-span-1 md:col-span-3 max-w-3xl mx-auto">
  
  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6">
    Abstract
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Modern conversational AI systems are often designed to use warm and empathetic language, but it remains unclear whether this stylistic choice <em>causally</em> affects users' emotional attachment. This project addresses that question through a large-scale causal inference study on real-world chatbot interactions.
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Using the WildChat-1M dataset, we operationalize empathy and attachment via an LLM-as-a-Judge framework and apply semantic matching and double robust learning to control for confounding factors. Our results show a statistically significant positive causal effect of empathetic responses on user attachment, with substantial heterogeneity across users and contexts.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Introduction
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    As large language models (LLMs) increasingly resemble human conversational partners, they blur the boundary between functional tools and social actors. Prior work suggests that empathetic AI behavior correlates with stronger user engagement, but correlation alone cannot establish causality.
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    This project investigates whether empathetic language produced by an LLM <em>causes</em> users to respond with higher emotional attachment, rather than simply co-occurring with it due to user vulnerability or contextual factors.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Related Work
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-4">
    The study builds on three main research threads:
  </p>
  
  <ul class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6 ml-6 list-disc">
    <li><strong>Human–AI social interaction</strong>, including the CASA paradigm and theories of artificial intimacy.</li>
    <li><strong>Empirical studies of parasocial relationships</strong>, showing that users can form emotional bonds with chatbots.</li>
    <li><strong>Causal inference with text</strong>, where high-dimensional language acts as a confounder and requires semantic controls.</li>
  </ul>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    While previous studies document emotional engagement with AI, they largely stop short of identifying empathy as a causal driver.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Problem Formulation
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-4">
    We model each interaction as a conversational turn-pair: a user prompt, an LLM response, and a user reply.
  </p>
  
  <ul class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6 ml-6 list-disc">
    <li><strong>Treatment (T):</strong> High vs. low empathy in the LLM response.</li>
    <li><strong>Outcome (Y):</strong> Emotional attachment expressed in the user's subsequent reply.</li>
    <li><strong>Confounders (X):</strong> Prompt semantics, latent user traits, conversation context, and model identity.</li>
  </ul>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    A causal graph formalizes these relationships and motivates adjustment strategies to block backdoor paths.
  </p>

</div>

<figure class="col-span-1 md:col-span-5 w-full md:w-[90%] mx-auto my-12">
    <img 
      src="/projects_images/causal_inference/DAG.png" 
      alt="Causal Graph" 
      class="w-full h-auto object-cover rounded-lg shadow-lg"
    />
</figure>

<div class="hidden md:block md:col-span-1"></div>

<div class="col-span-1 md:col-span-3 max-w-3xl mx-auto">

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Dataset and Preprocessing
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    We use the <strong>WildChat-1M</strong> dataset, containing over one million real-world chatbot conversations. A multi-stage preprocessing pipeline filters for English, non-transactional, non-toxic, multi-turn interactions. The final corpus consists of approximately 144k socio-emotional turn-pairs, from which a stratified sample of ~9k interactions is scored for empathy and attachment.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Methodology
  </h2>
  
  <h3 class="text-2xl font-serif font-bold text-gray-900 leading-tight mb-4 mt-8">
    LLM-as-a-Judge
  </h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Empathy and attachment are quantified on a 1–7 scale using a few-shot prompted LLM judge, allowing scalable annotation of subtle emotional cues.
  </p>
  
  <h3 class="text-2xl font-serif font-bold text-gray-900 leading-tight mb-4 mt-8">
    Matching-Based ATE Estimation
  </h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Semantic embeddings of user prompts are used to match treated and control samples via greedy and Hungarian matching. This enables estimation of the Average Treatment Effect (ATE) under strong semantic control.
  </p>
  
  <h3 class="text-2xl font-serif font-bold text-gray-900 leading-tight mb-4 mt-8">
    Double Robust Learning
  </h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    To validate robustness, a Double Robust (DR) Learner combines outcome modeling and propensity estimation while incorporating all confounders. This approach yields ATE, CATE, and HTE estimates and reduces sensitivity to model misspecification.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Results
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Across matching methods and DR learners, empathetic responses increase user attachment by approximately <strong>0.73–0.77 points</strong> on the attachment scale. While the average effect is stable and positive, heterogeneous treatment effect analysis reveals meaningful variation: most users benefit from empathetic language, while a minority respond neutrally or negatively.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Discussion
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    The convergence of multiple causal estimators provides strong evidence that empathetic AI language has a genuine causal impact on emotional attachment. Importantly, empathy is not universally beneficial—its effectiveness depends on user traits and conversational context. These findings highlight the need for adaptive, context-aware deployment of empathy in AI systems.
  </p>

  <h2 class="text-3xl md:text-4xl font-serif font-bold text-gray-900 leading-tight mb-6 mt-12">
    Future Work
  </h2>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Future directions include dynamic causal modeling across conversation turns, richer user-level covariates, alternative causal estimators (e.g., X-learner, TMLE), and human-annotated ground truth labels to strengthen external validity.
  </p>

</div>
