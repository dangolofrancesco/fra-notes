---
title: Does Empathetic AI Language Cause Emotional Attachment?
description: A causal inference study investigating whether empathetic language generated by large language models directly increases users' emotional attachment in real-world chatbot conversations.
image: /projects_images/causal_inference/llm-empathy-causal.jpg
tags: [Causal Inference, Large Language Models, Human-AI]
date: 2025-12-01
github: https://github.com/dangolofrancesco/llm-empathy-causal-study
report: /projects_images/causal_inference/Report.pdf
---

<div class="hidden md:block md:col-span-1"></div>

<div class="col-span-1 md:col-span-3 max-w-3xl mx-auto">
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    We assume that when an AI speaks kindly, users like it more. But correlation is not causation.
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Modern LLMs are fine-tuned to be warm and empathetic. But does this stylistic choice <em>actually cause</em> users to form an emotional attachment, or is it just a byproduct of users who are already lonely or vulnerable?
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    In traditional A/B testing, we randomize users. In observational data (like chat logs), we can't. If a user says "I'm sad," the AI <em>must</em> be empathetic. The treatment is confounded by the prompt. To solve this, I treated language as a high-dimensional causal problem.
  </p>

  <h3 class="text-2xl font-bold font-serif mb-4 mt-8">The Architecture: Measuring the Intangible</h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    The first challenge was measurement. You can't measure "emotional attachment" with a regex. I built a pipeline using the <strong>WildChat-1M</strong> dataset (over one million real interactions) and an <strong>LLM-as-a-Judge</strong> framework.
  </p>

</div>

<figure class="col-span-1 md:col-span-5 w-full md:w-[90%] mx-auto my-12">
    <img 
      src="/projects_images/causal_inference/DAG.png" 
      alt="DAG" 
      class="w-full h-auto object-cover rounded-lg shadow-lg"
    />
    <figcaption class="mt-4 text-center text-sm text-gray-500 font-serif italic">
      A Directed Acyclic Graph (DAG) of the hypothesized causal relationships.
    </figcaption>
</figure>

<div class="hidden md:block md:col-span-1"></div>

<div class="col-span-1 md:col-span-3 max-w-3xl mx-auto">

  <ul class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6 ml-6 list-disc">
    <li>
      <strong>Operationalizing Variables:</strong> I used a few-shot prompted LLM to score both "Model Empathy" (Treatment) and "User Attachment" (Outcome) on a robust 1-7 Likert scale.
    </li>
    <li>
      <strong>The Confounder Problem:</strong> A "Help me with code" prompt triggers low empathy. A "My dog died" prompt triggers high empathy. Comparing the two is apples-to-oranges.
    </li>
    <li>
      <strong>Semantic Matching:</strong> To fix this, I used embedding models to match users who said <em>semantically similar things</em> but received different levels of empathy from the AI.
    </li>
  </ul>

  <h3 class="text-2xl font-bold font-serif mb-4 mt-8">The Strategy: Double Robust Learning</h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    To estimate the true causal effect, I couldn't rely on a simple regression. I employed a <strong>Double Robust (DR) Learner</strong>. This method is the "belt and suspenders" of causal inference: it models both the probability of the treatment (Propensity Score) and the expected outcome.
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    If either model is correct, the final estimate is unbiased. The core estimation relies on the Average Treatment Effect (ATE) equation adjusted for confounders X:
  </p>

  <div class="my-8 text-center text-xl font-serif overflow-x-auto">

  $$
  ATE = E [ E[Y|T=1, X] - E[Y|T=0, X] ]
  $$

  </div>

   <h3 class="text-2xl font-bold font-serif mb-4 mt-8">The Results</h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    After controlling for prompt semantics, user traits, and conversation context, the results were clear. Empathetic responses caused a statistically significant increase in user attachment.
  </p>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Specifically, we observed an effect size of approximately <strong>+0.75 points</strong> on our 7-point scale. This confirms that empathy isn't just "nice to have", it's a mechanical lever for engagement.
  </p>

</div>

<figure class="col-span-1 md:col-span-5 w-full md:w-[60%] mx-auto my-12">
    <img 
      src="/projects_images/causal_inference/results.png" 
      alt="DR Learner Results" 
      class="w-full h-auto object-cover rounded-lg shadow-lg"
    />
    <figcaption class="mt-4 text-center text-sm text-gray-500 font-serif italic">
       DR-Learner causal effect estimates, controlling for all confounders.
    </figcaption>
</figure>

<div class="hidden md:block md:col-span-1"></div>

<div class="col-span-1 md:col-span-3 max-w-3xl mx-auto">

  <h3 class="text-2xl font-bold font-serif mb-4 mt-8">Lessons Learned</h3>
  
  <p class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6">
    Applying causal inference to unstructured text is messy. Here is what I took away:
  </p>

  <ol class="text-lg md:text-xl text-gray-800 leading-relaxed font-serif mb-6 ml-6 list-decimal">
    <li>
      <strong>Text is a High-Dimensional Confounder:</strong> You can't just control for "topic." Two prompts can be about "cooking" but have vastly different emotional valences. Embeddings are essential for proper matching.
    </li>
    <li>
      <strong>Heterogeneity Matters:</strong> The ATE (Average) hides the truth. While the average effect was positive, the <em>Heterogeneous Treatment Effect (HTE)</em> analysis showed that for some task-oriented users, empathy was actually a distraction.
    </li>
    <li>
      <strong>LLMs are Decent Judges:</strong> Using an LLM to score empathy proved scalable and surprisingly consistent with human intuition, provided the few-shot examples were carefully curated.
    </li>
  </ol>

  <blockquote class="my-10 pl-6 border-l-[4px] border-[#E85D04] text-xl italic font-serif text-gray-700">
    "We proved that AI empathy is not just a correlation. It is a causal driver of user connection, but it must be wielded with precision."
  </blockquote>

</div>
